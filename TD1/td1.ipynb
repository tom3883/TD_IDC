{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Bazm2ujK-y"
      },
      "source": [
        "\n",
        "# TD1 - Knowledge Extraction from text\n",
        "### *Nicolas AUDOUX - Thomas PAUL - Yana RAGOZINA*\n",
        "### SI5 IA-ID\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5pD-YR7o7Mg"
      },
      "source": [
        "In this lab we will be focusing on performing a keyword/keyphrase extraction analysis (KPE) on a datasest of documents. Therefore, we will try to build and evaluate different document summaries  generated by different keyphrase extraction algorithms.\n",
        "\n",
        "We will try to implement 3 different keyphrase extraction algorithms in order to analyse and compare their functionalities and performances.\n",
        "\n",
        "We will use one of the datasets designed for automatic keyphrase extraction, *Inspec*, collecting 2000 abstract documents in English language in the domain of Computer Science.\n",
        "\n",
        "As for the tested  KPE algorithms, we chose to analyse *PositionRank*, *SingleRank* and *TextRank*.\n",
        "\n",
        "The performances will be evaluated with ROUGE framework (Recall-Oriented Understudy for Gisting Evaluation), designed to compare an automatically produced summary  by the algorithms against a reference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux4pivY9jK-z"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "ut1PglfAjK-z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pke\n",
        "from rouge import Rouge\n",
        "from os import listdir\n",
        "from time import time\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "WVY4mpC0jK-0"
      },
      "outputs": [],
      "source": [
        "# Defining constants\n",
        "# pos and grammar for Position Rank\n",
        "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
        "grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
        "\n",
        "# Extractors\n",
        "position_rank_extractor = pke.unsupervised.PositionRank()\n",
        "single_rank_extractor = pke.unsupervised.SingleRank()\n",
        "text_rank_extractor = pke.unsupervised.TextRank()\n",
        "\n",
        "rouge = Rouge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "ZEWgBMtKjK-0"
      },
      "outputs": [],
      "source": [
        "def extract_keyphrases_score(extractor, doc, grammar=None, text_rank=None):\n",
        "   # load the content of the document\n",
        "   extractor.load_document(input=doc, language='en', normalization=None)\n",
        "\n",
        "   # select the noun phrases up to 3 words as keyphrase candidates\n",
        "   if grammar is not None:\n",
        "      extractor.candidate_selection(grammar=grammar, maximum_word_number=3)\n",
        "   else:\n",
        "      extractor.candidate_selection()\n",
        "\n",
        "   # weight the candidates using the sum of their word's scores that are\n",
        "   # computed using random walk biaised with the position of the words\n",
        "   # in the document. In the graph, nodes are words (nouns and\n",
        "   # adjectives only) that are connected if they occur in a window of\n",
        "   # 10 words.\n",
        "   if text_rank:\n",
        "      extractor.candidate_weighting(window=10, pos=pos, top_percent=0.33)\n",
        "   else:\n",
        "      extractor.candidate_weighting(window=10, pos=pos)\n",
        "\n",
        "   # get the 10-highest scored candidates as keyphrases\n",
        "   keyphrases = extractor.get_n_best(n=10)\n",
        "\n",
        "   # compute rouge scores\n",
        "   scores = rouge.get_scores(keyphrases[0][0], doc)\n",
        "\n",
        "   return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "TJF6KeIFjK-0"
      },
      "outputs": [],
      "source": [
        "def mean_all_scores(all_scores):\n",
        "    mean_r_1, mean_p_1, mean_f_1 = 0, 0, 0\n",
        "    mean_r_2, mean_p_2, mean_f_2 = 0, 0, 0\n",
        "    mean_r_l, mean_p_l, mean_f_l = 0, 0, 0\n",
        "\n",
        "    total_scores = len(all_scores)\n",
        "\n",
        "    for scores in all_scores:\n",
        "        mean_r_1 += scores['rouge-1']['r']\n",
        "        mean_p_1 += scores['rouge-1']['p']\n",
        "        mean_f_1 += scores['rouge-1']['f']\n",
        "\n",
        "        mean_r_2 += scores['rouge-2']['r']\n",
        "        mean_p_2 += scores['rouge-2']['p']\n",
        "        mean_f_2 += scores['rouge-2']['f']\n",
        "\n",
        "        mean_r_l += scores['rouge-l']['r']\n",
        "        mean_p_l += scores['rouge-l']['p']\n",
        "        mean_f_l += scores['rouge-l']['f']\n",
        "\n",
        "    mean_r_1 /= total_scores\n",
        "    mean_p_1 /= total_scores\n",
        "    mean_f_1 /= total_scores\n",
        "\n",
        "    mean_r_2 /= total_scores\n",
        "    mean_p_2 /= total_scores\n",
        "    mean_f_2 /= total_scores\n",
        "\n",
        "    mean_r_l /= total_scores\n",
        "    mean_p_l /= total_scores\n",
        "    mean_f_l /= total_scores\n",
        "\n",
        "    return mean_r_1, mean_p_1, mean_f_1, mean_r_2, mean_p_2, mean_f_2, mean_r_l, mean_p_l, mean_f_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "spBXJIvojK-0"
      },
      "outputs": [],
      "source": [
        "def get_scores(limitSize):\n",
        "    all_scores_pr, all_scores_sr, all_scores_tr  = [], [], []\n",
        "    duration_pr, duration_sr, duration_tr = 0, 0, 0\n",
        "\n",
        "    dir = \"Inspec/docsutf8/\"\n",
        "    directory = [dir+f for f in listdir(dir)][:limitSize]\n",
        "\n",
        "    for i in directory:\n",
        "        try:\n",
        "            with open(i) as inspec_file:\n",
        "                doc = inspec_file.read()\n",
        "            print(f\"Processing file {i}\", end='\\r')\n",
        "        except:\n",
        "            continue\n",
        "        t1 = time()\n",
        "        scores_pr = extract_keyphrases_score(position_rank_extractor, doc, grammar)\n",
        "        t2=time()\n",
        "        scores_sr = extract_keyphrases_score(single_rank_extractor, doc)\n",
        "        t3=time()\n",
        "        scores_tr = extract_keyphrases_score(text_rank_extractor, doc, text_rank=True)\n",
        "        t4=time()\n",
        "\n",
        "        duration_pr += t2-t1\n",
        "        duration_sr += t3-t2\n",
        "        duration_tr += t4-t3\n",
        "\n",
        "        if scores_pr != 0:\n",
        "            all_scores_pr.append(scores_pr[0])\n",
        "        if scores_sr != 0:\n",
        "            all_scores_sr.append(scores_sr[0])\n",
        "        if scores_tr != 0:\n",
        "            all_scores_tr.append(scores_tr[0])\n",
        "\n",
        "    return all_scores_pr, all_scores_sr, all_scores_tr, duration_pr, duration_sr, duration_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "7ZvsxPVKjK-1"
      },
      "outputs": [],
      "source": [
        "def print_scores(scores,duration):\n",
        "  mean_r_1, mean_p_1, mean_f_1, mean_r_2, mean_p_2, mean_f_2, mean_r_l, mean_p_l, mean_f_l = mean_all_scores(scores)\n",
        "  print(\"Suration (in seconds) :\",duration)\n",
        "  print(\"Mean_r_1:\", mean_r_1)\n",
        "  print(\"Mean_p_1:\", mean_p_1)\n",
        "  print(\"Mean_f_1:\", mean_f_1)\n",
        "\n",
        "  print(\"Mean_r_2:\", mean_r_2)\n",
        "  print(\"Mean_p_2:\", mean_p_2)\n",
        "  print(\"Mean_f_2:\", mean_f_2)\n",
        "\n",
        "  print(\"Mean_r_l:\", mean_r_l)\n",
        "  print(\"Mean_p_l:\", mean_p_l)\n",
        "  print(\"Mean_f_l:\", mean_f_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Return the keyphrase of the document\n",
        "def extract_keyphrases(extractor, doc, index, grammar=None, text_rank=None, pos=None):\n",
        "    try:\n",
        "        # load the content of the document\n",
        "        extractor.load_document(input=doc, language='en', normalization=None)\n",
        "\n",
        "        # select the noun phrases up to 3 words as keyphrase candidates\n",
        "        if grammar is not None:\n",
        "            extractor.candidate_selection(grammar=grammar, maximum_word_number=3)\n",
        "        else:\n",
        "            extractor.candidate_selection()\n",
        "\n",
        "        if text_rank:\n",
        "            extractor.candidate_weighting(window=10, pos=pos, top_percent=0.33)\n",
        "        else:\n",
        "            extractor.candidate_weighting(window=10, pos=pos)\n",
        "\n",
        "        # get the 10-highest scored candidates as keyphrases\n",
        "        keyphrases = extractor.get_n_best(n=10)\n",
        "\n",
        "        if keyphrases is not None and keyphrases:\n",
        "            res = f\"{index} : {keyphrases[0][0]}\"\n",
        "        else:\n",
        "            res = f\"{index} : None\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle specific exceptions if needed\n",
        "        res = f\"Error processing {index}: {str(e)}\"\n",
        "\n",
        "    # return the most predominant keyphrase\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_keyphrases(limitSize):\n",
        "    all_keyphrases_pr, all_keyphrases_sr, all_keyphrases_tr = [],[], []\n",
        "\n",
        "    dir = \"Inspec/docsutf8/\"\n",
        "    directory = [dir+f for f in listdir(dir)][:limitSize]\n",
        "\n",
        "    for i in directory:\n",
        "        try:\n",
        "            with open(i) as inspec_file:\n",
        "                doc = inspec_file.read()\n",
        "            print(f\"Processing file\",i, end='\\r')\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        keyphrase_pr = extract_keyphrases(position_rank_extractor, doc, i, grammar)\n",
        "        keyphrase_sr = extract_keyphrases(single_rank_extractor, doc,i)\n",
        "        keyphrase_tr = extract_keyphrases(text_rank_extractor, doc,i,text_rank=True)\n",
        "        \n",
        "        all_keyphrases_pr.append(keyphrase_pr)\n",
        "        all_keyphrases_sr.append(keyphrase_sr)\n",
        "        all_keyphrases_tr.append(keyphrase_tr)\n",
        "\n",
        "    return all_keyphrases_pr, all_keyphrases_sr, all_keyphrases_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract keyphrases of the first 5 documents\n",
        "all_keyphrases_pr, all_keyphrases_sr, all_keyphrases_tr = get_keyphrases(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Inspec/docsutf8/100.txt : separate accounts', 'Inspec/docsutf8/1000.txt : universality', 'Inspec/docsutf8/1001.txt : conflict ibs', 'Inspec/docsutf8/1002.txt : selective representing-the idea', 'Inspec/docsutf8/1003.txt : lob']\n",
            "['Inspec/docsutf8/100.txt : new entrants', 'Inspec/docsutf8/1000.txt : classical symbol systems', 'Inspec/docsutf8/1001.txt : content atomism', 'Inspec/docsutf8/1002.txt : selective representing-the idea', 'Inspec/docsutf8/1003.txt : human-like epistemic agents']\n",
            "['Inspec/docsutf8/100.txt : new entrants', 'Inspec/docsutf8/1000.txt : universality', 'Inspec/docsutf8/1001.txt : content atomism', 'Inspec/docsutf8/1002.txt : realist conception', 'Inspec/docsutf8/1003.txt : epistemic authority']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(all_keyphrases_pr)\n",
        "print(all_keyphrases_sr)\n",
        "print(all_keyphrases_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When comparing the different keyphrases extracted by our algorithms, we notice that single rank and text rank are very similar (we have the same keyphrases for the first 5 documents). Position rank has different results for most of the texts (not the 1002). However, even if the keyphrases are different, there mostly part of the keys phrases contained in the folder keys. \n",
        "If we take the 100.txt doc, it is quite reasonable to pick *separate accounts* as a keyphrases since its is part of the title and the main subject of the doc. *new entrants* seems less coherent on this document. For the second document (1000.txt), we have a similar result: *Universality* is the main topic of the document whereas *classical symbol systems* is just an example. For those two example the result of the PositionRank seems to be the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "OFFBhtDgjK-1",
        "outputId": "3b50d341-e888-40fe-8605-3e38dab4a8a1"
      },
      "outputs": [],
      "source": [
        "all_scores_pr, all_scores_sr, all_scores_tr, duration_pr, duration_sr, duration_tr = get_scores(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGIwtCGXjK-1"
      },
      "source": [
        "## 1. Position Rank\n",
        "\n",
        "Position Rank extracts keyphrases by determining the importance of a word based on its position in the document.\n",
        "\n",
        "It's an unsupervised algorithm that is decomposed like this :\n",
        "1. Calculates the Term Frequency of a word(TF)\n",
        "2. Adjusts the term frequency based on the length of the document (Document Length Normalization)\n",
        "3. Assigns scores to words based on their positions within sentences. **Words in the beggining and end of sentences have higher scores.** (Sentence Position Score)\n",
        "4. Combines the term frequency and sentence position scores to determine the overall importance of each word (Sentence Salience Score)\n",
        "5. Extracts words that have the highest salience scores (Keyphrase Extraction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "068UHfRqjK-1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suration (in seconds) : 198.8953652381897\n",
            "Mean_r_1: 0.02976155012249593\n",
            "Mean_p_1: 0.8816666666666667\n",
            "Mean_f_1: 0.057289591776589835\n",
            "Mean_r_2: 0.011282417062540108\n",
            "Mean_p_2: 0.73\n",
            "Mean_f_2: 0.02213746087937282\n",
            "Mean_r_l: 0.02976155012249593\n",
            "Mean_p_l: 0.8816666666666667\n",
            "Mean_f_l: 0.057289591776589835\n"
          ]
        }
      ],
      "source": [
        "print_scores(all_scores_pr,duration_pr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvil-Rg4jK-2"
      },
      "source": [
        "## 2. SingleRank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPobKzHBlGg_"
      },
      "source": [
        "SingleRank is a graph-based summarization algorithm. Therefore, it performs in the first place a graph representation of a given document where each sentence is represented as a node. Plus, this algorithm measures the degree of similarity between each pair of sentences of the document. Then, an importance degree (centrality) is assigned to each sentence, allowing a more precise analysis for building a summary. The further process of the summary generation can be seen as follows :\n",
        "\n",
        "- Calculating the similarity between each pair of sentences in the document, often based on the content overlap\n",
        "- Mapping the similar sentences as nodes to the graph where the measured similarities are used to build edges between the nodes.\n",
        "- Measuring the degree of importance of each node (sentence) within the graph\n",
        "- Ranking each sentence in descending order based on the previously calculated degree of importance (centrality)\n",
        "- Generating the summary of the document based on the highest-ranking sentences to form the summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "utz6UCTajK-2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suration (in seconds) : 195.1198558807373\n",
            "Mean_r_1: 0.03979474361445328\n",
            "Mean_p_1: 0.9201666666666667\n",
            "Mean_f_1: 0.07561336635076116\n",
            "Mean_r_2: 0.018064838153112422\n",
            "Mean_p_2: 0.8246666666666669\n",
            "Mean_f_2: 0.03501314950770341\n",
            "Mean_r_l: 0.03936969250975165\n",
            "Mean_p_l: 0.9131666666666667\n",
            "Mean_f_l: 0.07481193607998106\n"
          ]
        }
      ],
      "source": [
        "print_scores(all_scores_sr,duration_sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew839W_ojK-2"
      },
      "source": [
        "## 3. TextRank\n",
        "TextRank is an algorithm that identifies keywords by assessing their significance within a connected graph. It functions by analyzing the relationships between words or phrases to determine their importance in the context of the overall text.\n",
        "Here is he algorithm:\n",
        "* Tokenization and part of speech tagging\n",
        "* Reducing the number of words based on a syntactic filter (in our case we keep only noons propositions and adjectives)\n",
        "* With all the remainig words are added to the graph and an edge is craeted for every words that co-occur in a window of N words (in our case, N=10)\n",
        "\n",
        "At this point we have an undirected unweigth graph.\n",
        "\n",
        "* Then a initial value of 1 is set for every vertice\n",
        "* Finally a modify version of the PageRank algorithm is run to upgrade the vertice score.\n",
        "The main idea behind this algorithm is to give more importance to a word which is linked by many others. Moreover a link to word which is linked by many other is more important than a link to word which is linked to only one word. This is the same algorithm used to rankes web pages. The only difference is that we also use a weight to each wich corresponds to the co-occurence score\n",
        "* After that, we keep only a a third of our vertices which corresponds to the vertices which have the highest score.\n",
        "* A post processing is done on the remainng vertices and if two words appears next to each other in the document a multi-word keyword is created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "bE7IZrIvjK-2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Suration (in seconds) : 199.66671299934387\n",
            "Mean_r_1: 0.03242190965438641\n",
            "Mean_p_1: 0.9108333333333334\n",
            "Mean_f_1: 0.062183449477588165\n",
            "Mean_r_2: 0.013272283690595209\n",
            "Mean_p_2: 0.79\n",
            "Mean_f_2: 0.02591573531883978\n",
            "Mean_r_l: 0.03226806350054026\n",
            "Mean_p_l: 0.9075000000000002\n",
            "Mean_f_l: 0.06188933183052934\n"
          ]
        }
      ],
      "source": [
        "print_scores(all_scores_tr,duration_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VTzVJ7QjK-2"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "**Which algorithm got the best RED score?**\n",
        "\n",
        "From the 3 keyphrase extraction algorithms (Position Rank, Single Rank and Text Rank), Single Rank has the best ROUGE score.\n",
        "\n",
        "\n",
        "\n",
        "**How would you represent each document and its respective extracted key phrases in the form of a knowledge graph? What vocabulary would you use?**\n",
        "\n",
        "Low-level : each document would have a graph with nodes representing the extracted keyphrases.\n",
        "\n",
        "High-level : every documents would be represented as 1 node and would be linked by their predominant keyphrase extracted.\n",
        "\n",
        "We can use the vocabulary of the extracted keyphrases of all the documents.\n",
        "\n",
        "\n",
        "To represent this as a graph, we would have each document and each keyphrases as node. If a key phrase represents a document, we connect each other with a weighted link to measure the level of confidence. The vocabulary used can be all the extracted keyphrases. To have a more interesting graph, we could also compute the similarity between differents keyphrases to link them or link every keyphrases to a general topic."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ux4pivY9jK-z"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
