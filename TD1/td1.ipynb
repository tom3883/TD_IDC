{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Bazm2ujK-y"
      },
      "source": [
        "\n",
        "# TD1 - Knowledge Extraction from text\n",
        "### *Nicolas AUDOUX - Thomas PAUL - Yana RAGOZINA*\n",
        "### SI5 IA-ID\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab we will be focusing on performing a keyword/keyphrase extraction analysis (KPE) on a datasest of documents. Therefore, we will try to build and evaluate different document summaries  generated by different keyphrase extraction algorithms.\n",
        "\n",
        "We will try to implement 3 different keyphrase extraction algorithms in order to analyse and compare their functionalities and performances.\n",
        "\n",
        "We will use one of the datasets designed for automatic keyphrase extraction, *Inspec*, collecting 2000 abstract documents in English language in the domain of Computer Science.\n",
        "\n",
        "As for the tested  KPE algorithms, we chose to analyse *PositionRank*, *SingleRank* and *TextRank*.\n",
        "\n",
        "The performances will be evaluated with ROUGE framework (Recall-Oriented Understudy for Gisting Evaluation), designed to compare an automatically produced summary  by the algorithms against a reference."
      ],
      "metadata": {
        "id": "d5pD-YR7o7Mg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux4pivY9jK-z"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!pip install matplotlib\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oglH8Wxjuje",
        "outputId": "40c5dee8-6ef6-4b67-fd39-4ff50e5a04fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-kbkrk_tl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-kbkrk_tl\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.8.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.2.2)\n",
            "Collecting unidecode (from pke==2.0.0)\n",
            "  Downloading Unidecode-1.3.7-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (0.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.3.2)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.66.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (2023.6.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pke==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.3->pke==2.0.0) (0.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.3)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160628 sha256=86c4a188d9367579241f143da7f23e5cf4d356a51628d2f4868c89bc148f6ebf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gu25m4mq/wheels/8c/07/29/6b35bed2aa36e33d77ff3677eb716965ece4d2e56639ad0aab\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-2.0.0 unidecode-1.3.7\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "2023-11-26 12:16:30.167587: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-26 12:16:30.167653: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-26 12:16:30.167694: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-26 12:16:30.179258: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-26 12:16:31.465856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ut1PglfAjK-z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pke\n",
        "from rouge import Rouge\n",
        "from os import listdir\n",
        "from time import time\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WVY4mpC0jK-0"
      },
      "outputs": [],
      "source": [
        "# Defining constants\n",
        "# pos and grammar for Position Rank\n",
        "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
        "grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
        "\n",
        "# Extractors\n",
        "position_rank_extractor = pke.unsupervised.PositionRank()\n",
        "single_rank_extractor = pke.unsupervised.SingleRank()\n",
        "text_rank_extractor = pke.unsupervised.TextRank()\n",
        "\n",
        "rouge = Rouge()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZEWgBMtKjK-0"
      },
      "outputs": [],
      "source": [
        "def extract_keyphrases(extractor, doc, grammar=None, text_rank=None):\n",
        "   # load the content of the document\n",
        "   extractor.load_document(input=doc, language='en', normalization=None)\n",
        "\n",
        "   # select the noun phrases up to 3 words as keyphrase candidates\n",
        "   if grammar is not None:\n",
        "      extractor.candidate_selection(grammar=grammar, maximum_word_number=3)\n",
        "   else:\n",
        "      extractor.candidate_selection()\n",
        "\n",
        "   # weight the candidates using the sum of their word's scores that are\n",
        "   # computed using random walk biaised with the position of the words\n",
        "   # in the document. In the graph, nodes are words (nouns and\n",
        "   # adjectives only) that are connected if they occur in a window of\n",
        "   # 10 words.\n",
        "   if text_rank:\n",
        "      extractor.candidate_weighting(window=10, pos=pos, top_percent=0.33)\n",
        "   else:\n",
        "      extractor.candidate_weighting(window=10, pos=pos)\n",
        "\n",
        "   # get the 10-highest scored candidates as keyphrases\n",
        "   keyphrases = extractor.get_n_best(n=10)\n",
        "\n",
        "   # compute rouge scores\n",
        "   scores = rouge.get_scores(keyphrases[0][0], doc)\n",
        "\n",
        "   return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TJF6KeIFjK-0"
      },
      "outputs": [],
      "source": [
        "def mean_all_scores(all_scores):\n",
        "    mean_r_1, mean_p_1, mean_f_1 = 0, 0, 0\n",
        "    mean_r_2, mean_p_2, mean_f_2 = 0, 0, 0\n",
        "    mean_r_l, mean_p_l, mean_f_l = 0, 0, 0\n",
        "\n",
        "    total_scores = len(all_scores)\n",
        "\n",
        "    for scores in all_scores:\n",
        "        mean_r_1 += scores['rouge-1']['r']\n",
        "        mean_p_1 += scores['rouge-1']['p']\n",
        "        mean_f_1 += scores['rouge-1']['f']\n",
        "\n",
        "        mean_r_2 += scores['rouge-2']['r']\n",
        "        mean_p_2 += scores['rouge-2']['p']\n",
        "        mean_f_2 += scores['rouge-2']['f']\n",
        "\n",
        "        mean_r_l += scores['rouge-l']['r']\n",
        "        mean_p_l += scores['rouge-l']['p']\n",
        "        mean_f_l += scores['rouge-l']['f']\n",
        "\n",
        "    mean_r_1 /= total_scores\n",
        "    mean_p_1 /= total_scores\n",
        "    mean_f_1 /= total_scores\n",
        "\n",
        "    mean_r_2 /= total_scores\n",
        "    mean_p_2 /= total_scores\n",
        "    mean_f_2 /= total_scores\n",
        "\n",
        "    mean_r_l /= total_scores\n",
        "    mean_p_l /= total_scores\n",
        "    mean_f_l /= total_scores\n",
        "\n",
        "    return mean_r_1, mean_p_1, mean_f_1, mean_r_2, mean_p_2, mean_f_2, mean_r_l, mean_p_l, mean_f_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "spBXJIvojK-0"
      },
      "outputs": [],
      "source": [
        "def get_scores(limitSize):\n",
        "    all_scores_pr, all_scores_sr, all_scores_tr  = [], [], []\n",
        "    duration_pr, duration_sr, duration_tr = 0, 0, 0\n",
        "\n",
        "    dir = \"Inspec/docsutf8/\"\n",
        "    directory = [dir+f for f in listdir(dir)][:limitSize]\n",
        "\n",
        "    for i in directory:\n",
        "        try:\n",
        "            with open(i) as inspec_file:\n",
        "                doc = inspec_file.read()\n",
        "            print(f\"Processing file {i}\", end='\\r')\n",
        "        except:\n",
        "            continue\n",
        "        t1 = time()\n",
        "        scores_pr = extract_keyphrases(position_rank_extractor, doc, grammar)\n",
        "        t2=time()\n",
        "        scores_sr = extract_keyphrases(single_rank_extractor, doc)\n",
        "        t3=time()\n",
        "        scores_tr = extract_keyphrases(text_rank_extractor, doc, text_rank=True)\n",
        "        t4=time()\n",
        "\n",
        "        duration_pr += t2-t1\n",
        "        duration_sr += t3-t2\n",
        "        duration_tr += t4-t3\n",
        "\n",
        "        if scores_pr != 0:\n",
        "            all_scores_pr.append(scores_pr[0])\n",
        "        if scores_sr != 0:\n",
        "            all_scores_sr.append(scores_sr[0])\n",
        "        if scores_tr != 0:\n",
        "            all_scores_tr.append(scores_tr[0])\n",
        "\n",
        "    return all_scores_pr, all_scores_sr, all_scores_tr, duration_pr, duration_sr, duration_tr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7ZvsxPVKjK-1"
      },
      "outputs": [],
      "source": [
        "def print_scores(scores,duration):\n",
        "  mean_r_1, mean_p_1, mean_f_1, mean_r_2, mean_p_2, mean_f_2, mean_r_l, mean_p_l, mean_f_l = mean_all_scores(scores)\n",
        "  print(\"Suration (in seconds) :\",duration)\n",
        "  print(\"Mean_r_1:\", mean_r_1)\n",
        "  print(\"Mean_p_1:\", mean_p_1)\n",
        "  print(\"Mean_f_1:\", mean_f_1)\n",
        "\n",
        "  print(\"Mean_r_2:\", mean_r_2)\n",
        "  print(\"Mean_p_2:\", mean_p_2)\n",
        "  print(\"Mean_f_2:\", mean_f_2)\n",
        "\n",
        "  print(\"Mean_r_l:\", mean_r_l)\n",
        "  print(\"Mean_p_l:\", mean_p_l)\n",
        "  print(\"Mean_f_l:\", mean_f_l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "OFFBhtDgjK-1",
        "outputId": "3b50d341-e888-40fe-8605-3e38dab4a8a1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9c4e5ba71e56>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_scores_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_scores_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_scores_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration_pr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration_sr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-2c23c75fea52>\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(limitSize)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Inspec/docsutf8/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlimitSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Inspec/docsutf8/'"
          ]
        }
      ],
      "source": [
        "all_scores_pr, all_scores_sr, all_scores_tr, duration_pr, duration_sr, duration_tr = get_scores(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGIwtCGXjK-1"
      },
      "source": [
        "## 1. Position Rank\n",
        "\n",
        "Position Rank extracts keyphrases by determining the importance of a word based on its position in the document.\n",
        "\n",
        "It's an unsupervised algorithm that is decomposed like this :\n",
        "1. Calculates the Term Frequency of a word(TF)\n",
        "2. Adjusts the term frequency based on the length of the document (Document Length Normalization)\n",
        "3. Assigns scores to words based on their positions within sentences. **Words in the beggining and end of sentences have higher scores.** (Sentence Position Score)\n",
        "4. Combines the term frequency and sentence position scores to determine the overall importance of each word (Sentence Salience Score)\n",
        "5. Extracts words that have the highest salience scores (Keyphrase Extraction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "068UHfRqjK-1"
      },
      "outputs": [],
      "source": [
        "print_scores(all_scores_pr,duration_pr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvil-Rg4jK-2"
      },
      "source": [
        "## 2. SingleRank"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SingleRank is a graph-based summarization algorithm. Therefore, it performs in the first place a graph representation of a given document where each sentence is represented as a node. Plus, this algorithm measures the degree of similarity between each pair of sentences of the document. Then, an importance degree (centrality) is assigned to each sentence, allowing a more precise analysis for building a summary. The further process of the summary generation can be seen as follows :\n",
        "\n",
        "- Calculating the similarity between each pair of sentences in the document, often based on the content overlap\n",
        "- Mapping the similar sentences as nodes to the graph where the measured similarities are used to build edges between the nodes.\n",
        "- Measuring the degree of importance of each node (sentence) within the graph\n",
        "- Ranking each sentence in descending order based on the previously calculated degree of importance (centrality)\n",
        "- Generating the summary of the document based on the highest-ranking sentences to form the summary\n",
        "\n"
      ],
      "metadata": {
        "id": "fPobKzHBlGg_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utz6UCTajK-2"
      },
      "outputs": [],
      "source": [
        "print_scores(all_scores_sr,duration_sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew839W_ojK-2"
      },
      "source": [
        "## 3. TextRank\n",
        "TextRank is an algorithm that identifies keywords by assessing their significance within a connected graph. It functions by analyzing the relationships between words or phrases to determine their importance in the context of the overall text.\n",
        "Here is he algorithm:\n",
        "* Tokenization and part of speech tagging\n",
        "* Reducing the number of words based on a syntactic filter (in our case we keep only noons propositions and adjectives)\n",
        "* With all the remainig words are added to the graph and an edge is craeted for every words that co-occur in a window of N words (in our case, N=10)\n",
        "\n",
        "At this point we have an undirected unweigth graph.\n",
        "\n",
        "* Then a initial value of 1 is set for every vertice\n",
        "* Finally a modify version of the PageRank algorithm is run to upgrade the vertice score.\n",
        "The main idea behind this algorithm is to give more importance to a word which is linked by many others. Moreover a link to word which is linked by many other is more important than a link to word which is linked to only one word. This is the same algorithm used to rankes web pages. The only difference is that we also use a weight to each wich corresponds to the co-occurence score\n",
        "* After that, we keep only a a third of our vertices which corresponds to the vertices which have the highest score.\n",
        "* A post processing is done on the remainng vertices and if two words appears next to each other in the document a multi-word keyword is created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bE7IZrIvjK-2"
      },
      "outputs": [],
      "source": [
        "print_scores(all_scores_tr,duration_tr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VTzVJ7QjK-2"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "**Which algorithm got the best RED score?**\n",
        "\n",
        "From the 3 keyphrase extraction algorithms (Position Rank, Single Rank and Text Rank), Single Rank has the best ROUGE score.\n",
        "\n",
        "\n",
        "\n",
        "**How would you represent each document and its respective extracted key phrases in the form of a knowledge graph? What vocabulary would you use?**\n",
        "\n",
        "Low-level : each document would have a graph with nodes representing the extracted keyphrases.\n",
        "\n",
        "High-level : every documents would be represented as 1 node and would be linked by their predominant keyphrase extracted.\n",
        "\n",
        "We can use the vocabulary of the extracted keyphrases of all the documents.\n",
        "\n",
        "\n",
        "To represent this as a graph, we would have each document and each keyphrases as node. If a key phrase represents a document, we connect each other with a weighted link to measure the level of confidence. The vocabulary used can be all the extracted keyphrases. To have a more interesting graph, we could also compute the similarity between differents keyphrases to link them or link every keyphrases to a general topic."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ux4pivY9jK-z"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}