{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pke\n",
    "from rouge import Rouge\n",
    "from os import listdir\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "# pos and grammar for Position Rank\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
    "\n",
    "# Extractors\n",
    "position_rank_extractor = pke.unsupervised.PositionRank()\n",
    "single_rank_extractor = pke.unsupervised.SingleRank()\n",
    "text_rank_extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyphrases(extractor, doc, grammar=None, text_rank=None):\n",
    "   # load the content of the document\n",
    "   extractor.load_document(input=doc, language='en', normalization=None)\n",
    "\n",
    "   # select the noun phrases up to 3 words as keyphrase candidates\n",
    "   if grammar is not None:\n",
    "      extractor.candidate_selection(grammar=grammar, maximum_word_number=3)\n",
    "   else:\n",
    "      extractor.candidate_selection()\n",
    "\n",
    "   # weight the candidates using the sum of their word's scores that are\n",
    "   # computed using random walk biaised with the position of the words\n",
    "   # in the document. In the graph, nodes are words (nouns and\n",
    "   # adjectives only) that are connected if they occur in a window of\n",
    "   # 10 words.\n",
    "   if text_rank:\n",
    "      extractor.candidate_weighting(window=10, pos=pos, top_percent=0.33)\n",
    "   else:\n",
    "      extractor.candidate_weighting(window=10, pos=pos)\n",
    "\n",
    "   # get the 10-highest scored candidates as keyphrases\n",
    "   keyphrases = extractor.get_n_best(n=10)\n",
    "\n",
    "   # compute rouge scores\n",
    "   scores = rouge.get_scores(keyphrases[0][0], doc)\n",
    "\n",
    "   return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_all_scores(all_scores):\n",
    "    mean_r_1, mean_p_1, mean_f_1 = 0, 0, 0\n",
    "    mean_r_2, mean_p_2, mean_f_2 = 0, 0, 0\n",
    "    mean_r_l, mean_p_l, mean_f_l = 0, 0, 0\n",
    "\n",
    "    total_scores = len(all_scores)\n",
    "\n",
    "    for scores in all_scores:\n",
    "        mean_r_1 += scores['rouge-1']['r']\n",
    "        mean_p_1 += scores['rouge-1']['p']\n",
    "        mean_f_1 += scores['rouge-1']['f']\n",
    "\n",
    "        mean_r_2 += scores['rouge-2']['r']\n",
    "        mean_p_2 += scores['rouge-2']['p']\n",
    "        mean_f_2 += scores['rouge-2']['f']\n",
    "\n",
    "        mean_r_l += scores['rouge-l']['r']\n",
    "        mean_p_l += scores['rouge-l']['p']\n",
    "        mean_f_l += scores['rouge-l']['f']\n",
    "\n",
    "    mean_r_1 /= total_scores\n",
    "    mean_p_1 /= total_scores\n",
    "    mean_f_1 /= total_scores\n",
    "\n",
    "    mean_r_2 /= total_scores\n",
    "    mean_p_2 /= total_scores\n",
    "    mean_f_2 /= total_scores\n",
    "\n",
    "    mean_r_l /= total_scores\n",
    "    mean_p_l /= total_scores\n",
    "    mean_f_l /= total_scores\n",
    "\n",
    "    return mean_r_1, mean_p_1, mean_f_1, mean_r_2, mean_p_2, mean_f_2, mean_r_l, mean_p_l, mean_f_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(limitSize):\n",
    "    all_scores_pr, all_scores_sr, all_scores_tr  = [], [], []\n",
    "\n",
    "    dir = \"Inspec/docsutf8/\"\n",
    "    directory = [dir+f for f in listdir(dir)][:limitSize]\n",
    "\n",
    "    for i in directory:\n",
    "        try:\n",
    "            with open(i) as inspec_file:\n",
    "                doc = inspec_file.read()\n",
    "            print(f\"Processing file {i}\", end='\\r')\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        scores_pr = extract_keyphrases(position_rank_extractor, doc, grammar)\n",
    "        scores_sr = extract_keyphrases(single_rank_extractor, doc)\n",
    "        scores_tr = extract_keyphrases(text_rank_extractor, doc, text_rank=True)\n",
    "        \n",
    "        if scores_pr != 0:\n",
    "            all_scores_pr.append(scores_pr[0])\n",
    "        if scores_sr != 0:\n",
    "            all_scores_sr.append(scores_sr[0])\n",
    "        if scores_tr != 0:\n",
    "            all_scores_tr.append(scores_tr[0])\n",
    "\n",
    "    return all_scores_pr, all_scores_sr, all_scores_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(scores):\n",
    "  mean_r_1, mean_p_1, mean_f_1, mean_r_2, mean_p_2, mean_f_2, mean_r_l, mean_p_l, mean_f_l = mean_all_scores(scores)\n",
    "\n",
    "  print(\"Mean_r_1:\", mean_r_1)\n",
    "  print(\"Mean_p_1:\", mean_p_1)\n",
    "  print(\"Mean_f_1:\", mean_f_1)\n",
    "\n",
    "  print(\"Mean_r_2:\", mean_r_2)\n",
    "  print(\"Mean_p_2:\", mean_p_2)\n",
    "  print(\"Mean_f_2:\", mean_f_2)\n",
    "\n",
    "  print(\"Mean_r_l:\", mean_r_l)\n",
    "  print(\"Mean_p_l:\", mean_p_l)\n",
    "  print(\"Mean_f_l:\", mean_f_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores_pr, all_scores_sr, all_scores_tr = get_scores(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Position Rank\n",
    "\n",
    "Position Rank extracts keyphrases by determining the importance of a word based on its position in the document.\n",
    "\n",
    "It's an unsupervised algorithm that is decomposed like this :\n",
    "1. Calculates the Term Frequency of a word(TF)\n",
    "2. Adjusts the term frequency based on the length of the document (Document Length Normalization)\n",
    "3. Assigns scores to words based on their positions within sentences. **Words in the beggining and end of sentences have higher scores.** (Sentence Position Score)\n",
    "4. Combines the term frequency and sentence position scores to determine the overall importance of each word (Sentence Salience Score)\n",
    "5. Extracts words that have the highest salience scores (Keyphrase Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_r_1: 0.033937302699837475\n",
      "Mean_p_1: 0.8383333333333333\n",
      "Mean_f_1: 0.06473987120564263\n",
      "Mean_r_2: 0.011970391219890485\n",
      "Mean_p_2: 0.63\n",
      "Mean_f_2: 0.023372050642025913\n",
      "Mean_r_l: 0.03360076423829902\n",
      "Mean_p_l: 0.8299999999999998\n",
      "Mean_f_l: 0.06409295762539571\n"
     ]
    }
   ],
   "source": [
    "print_scores(all_scores_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_r_1: 0.04146125153142978\n",
      "Mean_p_1: 0.8483333333333336\n",
      "Mean_f_1: 0.0783200384985309\n",
      "Mean_r_2: 0.01798686206406106\n",
      "Mean_p_2: 0.7283333333333333\n",
      "Mean_f_2: 0.0348125409052189\n",
      "Mean_r_l: 0.04146125153142978\n",
      "Mean_p_l: 0.8483333333333336\n",
      "Mean_f_l: 0.0783200384985309\n"
     ]
    }
   ],
   "source": [
    "print_scores(all_scores_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TextRank\n",
    "TextRank is an algorithm that identifies keywords by assessing their significance within a connected graph. It functions by analyzing the relationships between words or phrases to determine their importance in the context of the overall text.\n",
    "Here is he algorithm:\n",
    "* Tokenization and part of speech tagging\n",
    "* Reducing the number of words based on a syntactic filter (in our case we keep only noons propositions and adjectives)\n",
    "* With all the remainig words are added to the graph and an edge is craeted for every words that co-occur in a window of N words (in our case, N=10)\n",
    "\n",
    "At this point we have an undirected unweigth graph.\n",
    "\n",
    "* Then a initial value of 1 is set for every vertice\n",
    "* Finally a modify version of the PageRank algorithm is run to upgrade the vertice score.\n",
    "The main idea behind this algorithm is to give more importance to a word which is linked by many others. Moreover a link to word which is linked by many other is more important than a link to word which is linked to only one word. This is the same algorithm used to rankes web pages. The only difference is that we also use a weight to each wich corresponds to the co-occurence score\n",
    "* After that, we keep only a a third of our vertices which corresponds to the vertices which have the highest score. \n",
    "* A post processing is done on the remainng vertices and if two words appears next to each other in the document a multi-word keyword is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_r_1: 0.03242645378398926\n",
      "Mean_p_1: 0.8416666666666667\n",
      "Mean_f_1: 0.06200805257246616\n",
      "Mean_r_2: 0.012227851350563715\n",
      "Mean_p_2: 0.6766666666666667\n",
      "Mean_f_2: 0.02390274002436832\n",
      "Mean_r_l: 0.03242645378398926\n",
      "Mean_p_l: 0.8416666666666667\n",
      "Mean_f_l: 0.06200805257246616\n"
     ]
    }
   ],
   "source": [
    "print_scores(all_scores_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "**Which algorithm got the best RED score?**\n",
    "\n",
    "From the 3 keyphrase extraction algorithms (Position Rank, Single Rank and Text Rank), Single Rank has the best ROUGE score.\n",
    "\n",
    "\n",
    "\n",
    "**How would you represent each document and its respective extracted key phrases in the form of a knowledge graph? What vocabulary would you use?**\n",
    "\n",
    "Low-level : each document would have a graph with nodes representing the extracted keyphrases.\n",
    "\n",
    "High-level : every documents would be represented as 1 node and would be linked by their predominant keyphrase extracted. \n",
    "\n",
    "We can use the vocabulary of the extracted keyphrases of all the documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
